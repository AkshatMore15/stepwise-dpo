---

## âœ… Day 1 Summary

We completed the full setup for the Stepwise DPO project:

- [x] Created a Python 3.11 virtual environment using `venv`
- [x] Installed PyTorch with MPS support (macOS ARM)
- [x] Verified `torch` + `mps` compatibility
- [x] Cloned the [PRM800k dataset](https://github.com/openai/prm800k) and fixed folder structure
- [x] Added `helpers.py` to load `.jsonl` format datasets
- [x] Project folder structure organized: `reward_model`, `trainer`, `experiments`, `utils`, `data`
- [x] GitHub connected and `.gitignore` updated
